{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886bf882",
   "metadata": {},
   "source": [
    "# Implementing and Comparing NoProp Variants: A Practical Guide\n",
    "\n",
    "After understanding the theory of **No Propagation** in our previous article, it's time to get our hands dirty and implement all three variants of NoProp methods:\n",
    "\n",
    "1. **NoProp-DT (Discrete Time)** - Uses fixed timesteps with a cosine noise schedule\n",
    "2. **NoProp-CT (Continuous Time)** - Uses continuous time with ODE solvers  \n",
    "3. **NoProp-FM (Flow Matching)** - Uses vector fields for smoother transitions\n",
    "\n",
    "In this notebook, we'll implement each variant from scratch, train them on MNIST, and compare their performance. Let's see how these revolutionary training methods perform in practice! 🚀\n",
    "\n",
    "## What Makes Each Variant Special?\n",
    "\n",
    "Before diving into code, let's understand what makes each variant unique:\n",
    "\n",
    "### 🎯 **NoProp-DT (Discrete Time)**\n",
    "- **Core Idea**: Fixed number of timesteps (T=10, T=20, etc.)\n",
    "- **Noise Schedule**: Precomputed cosine schedule \n",
    "- **Training**: Each timestep has its own denoising block\n",
    "- **Pros**: Simple, deterministic, easy to understand\n",
    "- **Cons**: Fixed resolution, less flexible\n",
    "\n",
    "### ⏰ **NoProp-CT (Continuous Time)**  \n",
    "- **Core Idea**: Continuous time parameter t ∈ [0,1]\n",
    "- **Noise Schedule**: Learnable continuous schedule\n",
    "- **Training**: Single block used at different time points\n",
    "- **Pros**: Flexible timesteps, smooth transitions\n",
    "- **Cons**: More complex, requires ODE solvers\n",
    "\n",
    "### 🌊 **NoProp-FM (Flow Matching)**\n",
    "- **Core Idea**: Learn vector fields for optimal transport\n",
    "- **Approach**: Direct path from noise to target\n",
    "- **Training**: Predict velocity vectors instead of denoised states\n",
    "- **Pros**: Most direct, theoretically elegant  \n",
    "- **Cons**: Different mathematical framework\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement each one and see how they compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0382f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for our NoProp implementations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device and random seed for reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "print(f\"📚 PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"📊 Dataset loaded:\")\n",
    "print(f\"   Train samples: {len(train_dataset)}\")\n",
    "print(f\"   Test samples: {len(test_dataset)}\")\n",
    "print(f\"   Image shape: {train_dataset[0][0].shape}\")\n",
    "print(f\"   Classes: 10 (digits 0-9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a0613",
   "metadata": {},
   "source": [
    "## Building Blocks: Shared Components\n",
    "\n",
    "Before implementing each variant, let's create the shared components that all NoProp methods use:\n",
    "\n",
    "1. **DenoiseBlock**: The core building block that learns to denoise representations\n",
    "2. **Utility Functions**: Helper functions for training and evaluation\n",
    "\n",
    "These components are inspired by diffusion models but adapted for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoiseBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The core denoising block used by all NoProp variants.\n",
    "    \n",
    "    This block takes:\n",
    "    - x: Input image features\n",
    "    - z: Current latent representation (potentially noisy)\n",
    "    - W_embed: Class embeddings\n",
    "    \n",
    "    And outputs:\n",
    "    - z_pred: Denoised/predicted latent representation\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN backbone for MNIST feature extraction\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 28x28 -> 14x14\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),  # 14x14 -> 7x7\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # 7x7 -> 1x1\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Latent processing\n",
    "        self.z_processor = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Fusion and output\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(256 + 256, 256),  # x_features + z_features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embedding_dim)  # Output denoised z\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, z, W_embed):\n",
    "        \"\"\"\n",
    "        Forward pass of the denoising block.\n",
    "        \n",
    "        Args:\n",
    "            x: Input images [batch_size, 1, 28, 28]\n",
    "            z: Current latent state [batch_size, embedding_dim] \n",
    "            W_embed: Class embeddings [num_classes, embedding_dim]\n",
    "            \n",
    "        Returns:\n",
    "            z_pred: Predicted/denoised latent [batch_size, embedding_dim]\n",
    "            None: Placeholder for compatibility\n",
    "        \"\"\"\n",
    "        # Extract image features\n",
    "        x_features = self.backbone(x)\n",
    "        \n",
    "        # Process current latent state\n",
    "        z_features = self.z_processor(z)\n",
    "        \n",
    "        # Fuse and predict denoised latent\n",
    "        combined = torch.cat([x_features, z_features], dim=1)\n",
    "        z_pred = self.fusion(combined)\n",
    "        \n",
    "        return z_pred, None\n",
    "\n",
    "# Test the denoising block\n",
    "print(\"🔧 Testing DenoiseBlock...\")\n",
    "test_block = DenoiseBlock(embedding_dim=512, num_classes=10).to(device)\n",
    "test_x = torch.randn(4, 1, 28, 28).to(device)\n",
    "test_z = torch.randn(4, 512).to(device)\n",
    "test_W = torch.randn(10, 512).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_pred, _ = test_block(test_x, test_z, test_W)\n",
    "    print(f\"✅ DenoiseBlock works! Input: {test_z.shape} → Output: {z_pred.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in test_block.parameters())\n",
    "print(f\"📊 DenoiseBlock parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444915d",
   "metadata": {},
   "source": [
    "## The NoProp Training Flow: A Bird's Eye View\n",
    "\n",
    "Before diving into specific implementations, let's understand the **core training flow** that all NoProp variants follow:\n",
    "\n",
    "```\n",
    "Label y\n",
    "   │\n",
    "   ▼\n",
    "Sample z_T ~ q(z_T | y)                           ← Start from label\n",
    "   ↓\n",
    "Sample z_{T-1}, ..., z_0 ~ q(· | z_{t+1})        ← Reverse path (inference model)\n",
    "   ↓\n",
    "Forward z_0 → z_1 → ... → z_T using p(z_t | z_{t-1}, x)  ← Generative model\n",
    "   ↓\n",
    "Predict ŷ from z_T                                ← Classification\n",
    "   ↓\n",
    "Compute NoProp loss (ELBO):                       ← Training objective\n",
    "   - Cross-entropy (from output)\n",
    "   - KL divergence (z_0) \n",
    "   - L2 loss per block\n",
    "   ↓\n",
    "Update weights locally                            ← No backpropagation!\n",
    "```\n",
    "\n",
    "### 🔍 **Breaking Down Each Step:**\n",
    "\n",
    "1. **🎯 Start with Label y**: We begin with the ground truth class label\n",
    "2. **📤 Sample z_T**: Generate the final latent representation from class embedding  \n",
    "3. **🔄 Reverse Sampling**: Work backwards to sample the entire latent trajectory\n",
    "4. **➡️ Forward Pass**: Run the generative model forward using input x\n",
    "5. **🎲 Predict**: Make final classification from z_T\n",
    "6. **📊 Compute Loss**: ELBO with three components (CE + KL + L2)\n",
    "7. **⚡ Local Updates**: Each block updates independently - **no global backprop!**\n",
    "\n",
    "### 🤔 **Why This Works:**\n",
    "\n",
    "- **🎨 Latent Trajectory**: Each z_t represents the \"thought process\" at timestep t\n",
    "- **🎯 Target-Driven**: We start from where we want to end up (the correct class)\n",
    "- **🔧 Local Learning**: Each block learns its own denoising/prediction task\n",
    "- **📈 ELBO Objective**: Ensures the generative and inference models stay aligned\n",
    "\n",
    "This is fundamentally different from backpropagation where errors flow backwards. Here, each layer learns to map from its current state toward the target representation!\n",
    "\n",
    "---\n",
    "\n",
    "Now let's see how each variant implements this flow differently..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013758ae",
   "metadata": {},
   "source": [
    "## Key Differences: It's More Than Just Discrete vs Continuous!\n",
    "\n",
    "You're right to ask - while **DT vs CT** is about discrete vs continuous time, **Flow Matching (FM)** is fundamentally different. Let me break down the core differences:\n",
    "\n",
    "### 🔍 **Three Different Paradigms:**\n",
    "\n",
    "| Aspect | NoProp-DT | NoProp-CT | NoProp-FM |\n",
    "|--------|-----------|-----------|-----------|\n",
    "| **Core Idea** | Discrete denoising | Continuous denoising | **Vector field learning** |\n",
    "| **Time** | Fixed steps (T=10) | Continuous t∈[0,1] | Continuous t∈[0,1] |\n",
    "| **Training Target** | 🎯 **Predict clean u_y** | 🎯 **Predict clean u_y** | 🌊 **Predict velocity v** |\n",
    "| **Path Type** | Noise schedule | Noise schedule | **Straight lines** |\n",
    "| **Math Framework** | Diffusion | Diffusion/SDE | **Optimal Transport** |\n",
    "\n",
    "### 🧮 **The Mathematical Differences:**\n",
    "\n",
    "#### **DT & CT (Denoising Paradigm):**\n",
    "```\n",
    "Training: Learn f(x, z_noisy, t) → z_clean\n",
    "Goal: Remove noise from z_t to get u_y\n",
    "Path: z_t = √α̅(t) × u_y + √(1-α̅(t)) × noise\n",
    "```\n",
    "\n",
    "#### **FM (Flow Matching Paradigm):**\n",
    "```\n",
    "Training: Learn v(x, z_t, t) → velocity  \n",
    "Goal: Predict direction to move in latent space\n",
    "Path: z_t = t×z₁ + (1-t)×z₀  (straight line!)\n",
    "```\n",
    "\n",
    "### 🎯 **Training Process Comparison:**\n",
    "\n",
    "#### **NoProp-DT/CT (Denoising):**\n",
    "1. Start with clean target: u_y (class embedding)\n",
    "2. **Add noise**: z_t = √α̅(t) × u_y + √(1-α̅(t)) × ε\n",
    "3. **Train to denoise**: Predict u_y from (x, z_t, t)\n",
    "4. **Loss**: L2(predicted_clean, actual_clean)\n",
    "\n",
    "#### **NoProp-FM (Flow Matching):**\n",
    "1. Start with noise z₀ and target z₁ = u_y  \n",
    "2. **Linear interpolation**: z_t = t×z₁ + (1-t)×z₀\n",
    "3. **Train to predict velocity**: v* = z₁ - z₀ (constant!)\n",
    "4. **Loss**: L2(predicted_velocity, target_velocity)\n",
    "\n",
    "### 🚀 **Why These Differences Matter:**\n",
    "\n",
    "#### **🔧 Implementation Complexity:**\n",
    "- **DT**: Most parameters (T blocks), but simplest math\n",
    "- **CT**: Fewest parameters (1 block), needs ODE solvers  \n",
    "- **FM**: Medium complexity, but no noise schedules needed\n",
    "\n",
    "#### **🧠 Mathematical Elegance:**\n",
    "- **DT/CT**: Based on diffusion models (complex noise schedules)\n",
    "- **FM**: Based on optimal transport (straight paths are optimal!)\n",
    "\n",
    "#### **⚡ Training Efficiency:**\n",
    "- **DT**: Must train all T timesteps per batch\n",
    "- **CT**: Sample random t, train one denoising step\n",
    "- **FM**: Sample random t, train one velocity prediction\n",
    "\n",
    "#### **🎨 Latent Space Behavior:**\n",
    "- **DT/CT**: Curved paths through noise → clean\n",
    "- **FM**: Straight paths from noise → target (mathematically optimal!)\n",
    "\n",
    "### 💡 **The Key Insight:**\n",
    "\n",
    "**Flow Matching is NOT just \"continuous denoising\"** - it's a completely different approach:\n",
    "\n",
    "- **Denoising methods** (DT/CT): *\"How do I remove noise to get the clean signal?\"*\n",
    "- **Flow Matching** (FM): *\"What's the optimal path from noise to target?\"*\n",
    "\n",
    "FM sidesteps the entire concept of \"noise removal\" and instead learns the **optimal transport map** directly! 🌊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cdfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 CODE COMPARISON: See the core differences in action!\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🎯 TRAINING STEP COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def show_dt_training():\n",
    "    print(\"\\n🔢 NoProp-DT Training (Discrete Denoising):\")\n",
    "    print(\"```python\")\n",
    "    print(\"# For each discrete timestep t:\")\n",
    "    print(\"for t in range(T):\")\n",
    "    print(\"    alpha_bar_t = self.alpha_bar[t]  # Fixed schedule\")\n",
    "    print(\"    \")\n",
    "    print(\"    # Add noise to clean target\")\n",
    "    print(\"    noise = torch.randn_like(u_y)\")\n",
    "    print(\"    z_t = sqrt(alpha_bar_t) * u_y + sqrt(1-alpha_bar_t) * noise\")\n",
    "    print(\"    \")\n",
    "    print(\"    # Train to DENOISE: predict clean from noisy\")\n",
    "    print(\"    z_pred = self.blocks[t](x, z_t, W_embed)\")\n",
    "    print(\"    loss = MSE(z_pred, u_y)  # Predict clean target\")\n",
    "    print(\"```\")\n",
    "\n",
    "def show_ct_training():\n",
    "    print(\"\\n⏰ NoProp-CT Training (Continuous Denoising):\")\n",
    "    print(\"```python\") \n",
    "    print(\"# Sample random continuous time\")\n",
    "    print(\"t = torch.rand(B, 1)  # t ∈ [0,1]\")\n",
    "    print(\"alpha_bar_t = cos²(t × π/2)  # Continuous schedule\")\n",
    "    print(\"\")\n",
    "    print(\"# Add noise to clean target\")\n",
    "    print(\"noise = torch.randn_like(u_y)\")\n",
    "    print(\"z_t = sqrt(alpha_bar_t) * u_y + sqrt(1-alpha_bar_t) * noise\")\n",
    "    print(\"\")\n",
    "    print(\"# Train to DENOISE: predict clean from noisy\")\n",
    "    print(\"z_pred = self.block(x, z_t, W_embed)\")\n",
    "    print(\"loss = MSE(z_pred, u_y)  # Predict clean target\")\n",
    "    print(\"```\")\n",
    "\n",
    "def show_fm_training():\n",
    "    print(\"\\n🌊 NoProp-FM Training (Flow Matching - DIFFERENT!):\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Sample random time and create straight-line path\")\n",
    "    print(\"t = torch.rand(B, 1)  # t ∈ [0,1]\")\n",
    "    print(\"z0 = torch.randn_like(u_y)  # Random start\")\n",
    "    print(\"z1 = u_y  # Target end\")\n",
    "    print(\"\")\n",
    "    print(\"# Linear interpolation (NO noise schedule!)\")\n",
    "    print(\"z_t = t * z1 + (1-t) * z0  # Straight line!\")\n",
    "    print(\"\")\n",
    "    print(\"# Train to predict VELOCITY (not denoised state!)\")\n",
    "    print(\"v_target = z1 - z0  # Constant velocity\")\n",
    "    print(\"v_pred = self.vector_field(x, z_t, W_embed)\")\n",
    "    print(\"loss = MSE(v_pred, v_target)  # Predict velocity!\")\n",
    "    print(\"```\")\n",
    "\n",
    "show_dt_training()\n",
    "show_ct_training() \n",
    "show_fm_training()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔑 KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ DT vs CT: Same paradigm (denoising), different time discretization\")\n",
    "print(\"✅ FM: Completely different paradigm (optimal transport)\")\n",
    "print(\"✅ DT/CT: Predict clean states from noisy states\")\n",
    "print(\"✅ FM: Predict velocities for optimal paths\")\n",
    "print(\"✅ DT/CT: Curved paths through noise space\")\n",
    "print(\"✅ FM: Straight paths (mathematically optimal!)\")\n",
    "\n",
    "# Visual comparison of what each method learns\n",
    "print(\"\\n🎨 VISUAL INTUITION:\")\n",
    "print(\"DT/CT: noise ~~~> clean  (remove corruption)\")\n",
    "print(\"FM:    start ──→ target  (optimal transport)\")\n",
    "\n",
    "print(\"\\n🧮 MATHEMATICAL FOUNDATION:\")\n",
    "print(\"DT/CT: Diffusion models (Brownian motion, SDEs)\")\n",
    "print(\"FM:    Optimal transport (Wasserstein distance, straight paths)\")\n",
    "\n",
    "print(\"\\n⚡ EFFICIENCY:\")\n",
    "print(\"DT:    T training steps per batch (expensive)\")\n",
    "print(\"CT:    1 training step per batch (efficient)\")\n",
    "print(\"FM:    1 training step per batch (efficient + no noise schedule!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9cff3",
   "metadata": {},
   "source": [
    "## Implementation 1: NoProp-DT (Discrete Time)\n",
    "\n",
    "The **Discrete Time** variant is the most straightforward to understand. Here's how it works:\n",
    "\n",
    "### 🎯 **Key Concepts:**\n",
    "\n",
    "1. **Fixed Timesteps**: We use T discrete timesteps (e.g., T=10)\n",
    "2. **Cosine Schedule**: Noise level follows a cosine schedule: α̅(t) = cos²(t/T × π/2)\n",
    "3. **Stacked Blocks**: Each timestep has its own denoising block\n",
    "4. **Training**: For each timestep t, we:\n",
    "   - Sample noisy latent: z_t = √α̅(t) × u_y + √(1-α̅(t)) × noise\n",
    "   - Train block to predict clean u_y from z_t\n",
    "\n",
    "### 📊 **Loss Function:**\n",
    "The discrete time loss combines three terms:\n",
    "- **Classification loss**: Cross-entropy at final timestep\n",
    "- **KL loss**: Regularizes latent space\n",
    "- **Denoising loss**: SNR-weighted L2 loss per timestep\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPropDT(nn.Module):\n",
    "    \"\"\"\n",
    "    NoProp Discrete Time implementation.\n",
    "    \n",
    "    Uses T fixed timesteps with precomputed cosine noise schedule.\n",
    "    Each timestep has its own denoising block.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, embedding_dim: int, T: int = 10, eta: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.T = T\n",
    "        self.eta = eta\n",
    "\n",
    "        # Stack of T denoising blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DenoiseBlock(embedding_dim, num_classes) for _ in range(T)\n",
    "        ])\n",
    "\n",
    "        # Learnable class embeddings (this is where the magic happens!)\n",
    "        self.W_embed = nn.Parameter(torch.randn(num_classes, embedding_dim) * 0.1)\n",
    "\n",
    "        # Final classifier head\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "        # Precompute cosine noise schedule\n",
    "        t = torch.arange(1, T+1, dtype=torch.float32)\n",
    "        alpha_t = torch.cos(t / T * (torch.pi / 2)).pow(2)\n",
    "        alpha_bar = torch.cumprod(alpha_t, dim=0)\n",
    "\n",
    "        # SNR deltas for loss weighting (this weights each timestep appropriately)\n",
    "        snr = alpha_bar / (1 - alpha_bar + 1e-8)  # Avoid division by zero\n",
    "        snr_prev = torch.cat([torch.tensor([0.], dtype=snr.dtype), snr[:-1]], dim=0)\n",
    "        snr_delta = snr - snr_prev\n",
    "\n",
    "        # Register as buffers so they move with .to(device)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "        self.register_buffer('snr_delta', snr_delta)\n",
    "\n",
    "    def inference(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inference: Sequential denoising from pure noise to class prediction.\n",
    "        \n",
    "        This is the forward pass during testing - we start with pure noise\n",
    "        and progressively denoise through all T timesteps.\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        z = torch.randn(B, self.embedding_dim, device=x.device)\n",
    "        \n",
    "        # Sequential denoising through all blocks\n",
    "        for t in range(self.T):\n",
    "            z, _ = self.blocks[t](x, z, self.W_embed)\n",
    "        \n",
    "        # Final classification\n",
    "        return self.classifier(z)\n",
    "\n",
    "    def training_step(self, x, y):\n",
    "        \"\"\"\n",
    "        Training step for discrete time NoProp.\n",
    "        \n",
    "        For each timestep, we:\n",
    "        1. Get target class embedding u_y\n",
    "        2. Add noise according to schedule: z_t = √α̅(t) × u_y + √(1-α̅(t)) × noise\n",
    "        3. Train block to predict u_y from (x, z_t)\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Get target class embeddings\n",
    "        u_y = self.W_embed[y]  # [B, embedding_dim]\n",
    "        \n",
    "        # Train each timestep\n",
    "        for t in range(self.T):\n",
    "            # Get noise schedule values\n",
    "            alpha_bar_t = self.alpha_bar[t]\n",
    "            \n",
    "            # Sample noise and create noisy latent\n",
    "            noise = torch.randn_like(u_y)\n",
    "            z_t = torch.sqrt(alpha_bar_t) * u_y + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "            \n",
    "            # Predict clean latent\n",
    "            z_pred, _ = self.blocks[t](x, z_t, self.W_embed)\n",
    "            \n",
    "            # L2 denoising loss weighted by SNR\n",
    "            loss_l2 = F.mse_loss(z_pred, u_y)\n",
    "            loss = 0.5 * self.eta * self.snr_delta[t] * loss_l2\n",
    "            \n",
    "            # Add classification and KL losses for final timestep\n",
    "            if t == self.T - 1:\n",
    "                logits = self.classifier(z_pred)\n",
    "                loss_ce = F.cross_entropy(logits, y)\n",
    "                loss_kl = 0.5 * u_y.pow(2).sum(dim=1).mean()  # Simple KL regularization\n",
    "                loss = loss + loss_ce + loss_kl\n",
    "            \n",
    "            total_loss += loss\n",
    "            \n",
    "        return total_loss\n",
    "\n",
    "# Create and test NoProp-DT model\n",
    "print(\"🏗️  Creating NoProp-DT model...\")\n",
    "model_dt = NoPropDT(num_classes=10, embedding_dim=512, T=10, eta=0.1).to(device)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_x = torch.randn(4, 1, 28, 28).to(device)\n",
    "    test_y = torch.randint(0, 10, (4,)).to(device)\n",
    "    \n",
    "    # Test inference\n",
    "    logits = model_dt.inference(test_x)\n",
    "    print(f\"✅ Inference works! Output shape: {logits.shape}\")\n",
    "    \n",
    "    # Test training step\n",
    "    loss = model_dt.training_step(test_x, test_y)\n",
    "    print(f\"✅ Training step works! Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.numel() for p in model_dt.parameters())\n",
    "print(f\"📊 NoProp-DT parameters: {total_params:,}\")\n",
    "print(f\"🔄 Timesteps: {model_dt.T}\")\n",
    "print(f\"📏 Embedding dimension: {model_dt.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61522843",
   "metadata": {},
   "source": [
    "## Implementation 2: NoProp-CT (Continuous Time)\n",
    "\n",
    "The **Continuous Time** variant is more flexible and mathematically elegant. Instead of fixed timesteps, it uses continuous time t ∈ [0,1].\n",
    "\n",
    "### ⏰ **Key Concepts:**\n",
    "\n",
    "1. **Continuous Time**: Sample t uniformly from [0,1] during training\n",
    "2. **Single Block**: One shared denoising block used at all timesteps\n",
    "3. **ODE Framework**: Training as solving an ODE with score matching\n",
    "4. **Flexible Inference**: Can use any number of steps during inference\n",
    "\n",
    "### 🔄 **Training Process:**\n",
    "1. Sample random time t ∼ Uniform[0,1]\n",
    "2. Create noisy latent: z_t = √α̅(t) × u_y + √(1-α̅(t)) × noise  \n",
    "3. Train block to predict u_y from (x, z_t, t)\n",
    "4. Weight loss by SNR'(t) for optimal convergence\n",
    "\n",
    "### 🧮 **Mathematical Foundation:**\n",
    "This is based on continuous-time diffusion models where we learn the score function ∇log p_t(z_t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPropCT(nn.Module):\n",
    "    \"\"\"\n",
    "    NoProp Continuous Time implementation.\n",
    "    \n",
    "    Uses continuous time t ∈ [0,1] with a single shared denoising block.\n",
    "    More flexible than discrete time version.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, embedding_dim: int, eta: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.eta = eta\n",
    "\n",
    "        # Single shared denoising block (used at all timesteps)\n",
    "        self.block = DenoiseBlock(embedding_dim, num_classes)\n",
    "        \n",
    "        # Learnable class embeddings\n",
    "        self.W_embed = nn.Parameter(torch.randn(num_classes, embedding_dim) * 0.1)\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def alpha_bar(self, t):\n",
    "        \"\"\"\n",
    "        Cosine noise schedule: α̅(t) = cos²(t × π/2)\n",
    "        \n",
    "        This controls how much signal vs noise we have at time t:\n",
    "        - t=0: α̅(0)=1 (pure signal, no noise)\n",
    "        - t=1: α̅(1)=0 (pure noise, no signal)\n",
    "        \"\"\"\n",
    "        return torch.cos(t * torch.pi / 2).pow(2)\n",
    "    \n",
    "    def snr_prime(self, t):\n",
    "        \"\"\"\n",
    "        Derivative of Signal-to-Noise Ratio for loss weighting.\n",
    "        \n",
    "        This ensures the loss is weighted properly across all timesteps,\n",
    "        giving more weight to harder denoising steps.\n",
    "        \"\"\"\n",
    "        alpha_bar_t = self.alpha_bar(t)\n",
    "        return 2 * alpha_bar_t / (1 - alpha_bar_t + 1e-8).pow(2)\n",
    "\n",
    "    def forward_denoise(self, x, z_t, t):\n",
    "        \"\"\"\n",
    "        Forward denoising step.\n",
    "        \n",
    "        Note: In continuous time, we pass the same block but it learns\n",
    "        to handle different noise levels based on the context.\n",
    "        \"\"\"\n",
    "        z_pred, _ = self.block(x, z_t, self.W_embed)\n",
    "        return z_pred\n",
    "\n",
    "    def inference(self, x: torch.Tensor, steps: int = 100) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inference using Euler method ODE solver.\n",
    "        \n",
    "        We solve the ODE: dz/dt = f(z,t) where f is learned by our model.\n",
    "        Start from pure noise and integrate to get clean representation.\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        z = torch.randn(B, self.embedding_dim, device=x.device)\n",
    "        \n",
    "        dt = 1.0 / steps\n",
    "        for i in range(steps):\n",
    "            t = torch.full((B, 1), i * dt, device=x.device)\n",
    "            \n",
    "            # Predict clean representation\n",
    "            z_pred = self.forward_denoise(x, z, t)\n",
    "            \n",
    "            # Euler step: z_{t+dt} = z_t + dt × (z_pred - z_t)\n",
    "            z = z + dt * (z_pred - z)\n",
    "        \n",
    "        return self.classifier(z)\n",
    "\n",
    "    def training_step(self, x, y):\n",
    "        \"\"\"\n",
    "        Training step for continuous time NoProp.\n",
    "        \n",
    "        Much simpler than discrete time - we just sample random t\n",
    "        and train to denoise at that timestep.\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        \n",
    "        # Get target class embeddings\n",
    "        u_y = self.W_embed[y]\n",
    "\n",
    "        # Sample random continuous time\n",
    "        t = torch.rand(B, 1, device=x.device)\n",
    "\n",
    "        # Create noisy latent according to schedule\n",
    "        alpha_bar_t = self.alpha_bar(t)\n",
    "        noise = torch.randn_like(u_y)\n",
    "        z_t = torch.sqrt(alpha_bar_t) * u_y + torch.sqrt(1 - alpha_bar_t) * noise\n",
    "\n",
    "        # Predict clean latent\n",
    "        z_pred = self.forward_denoise(x, z_t, t)\n",
    "        \n",
    "        # Compute losses\n",
    "        snr_prime_t = self.snr_prime(t)\n",
    "        loss_l2 = F.mse_loss(z_pred, u_y)\n",
    "        loss = 0.5 * self.eta * snr_prime_t.mean() * loss_l2\n",
    "\n",
    "        # Add classification and KL losses\n",
    "        logits = self.classifier(z_pred)\n",
    "        loss_ce = F.cross_entropy(logits, y)\n",
    "        loss_kl = 0.5 * u_y.pow(2).sum(dim=1).mean()\n",
    "        \n",
    "        total_loss = loss + loss_ce + loss_kl\n",
    "        return total_loss\n",
    "\n",
    "# Create and test NoProp-CT model\n",
    "print(\"🏗️  Creating NoProp-CT model...\")\n",
    "model_ct = NoPropCT(num_classes=10, embedding_dim=512, eta=1.0).to(device)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_x = torch.randn(4, 1, 28, 28).to(device)\n",
    "    test_y = torch.randint(0, 10, (4,)).to(device)\n",
    "    \n",
    "    # Test inference with different step counts\n",
    "    for steps in [10, 50, 100]:\n",
    "        logits = model_ct.inference(test_x, steps=steps)\n",
    "        print(f\"✅ Inference ({steps} steps) works! Output shape: {logits.shape}\")\n",
    "    \n",
    "    # Test training step\n",
    "    loss = model_ct.training_step(test_x, test_y)\n",
    "    print(f\"✅ Training step works! Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.numel() for p in model_ct.parameters())\n",
    "print(f\"📊 NoProp-CT parameters: {total_params:,}\")\n",
    "print(f\"🔄 Uses continuous time t ∈ [0,1]\")\n",
    "print(f\"📏 Embedding dimension: {model_ct.embedding_dim}\")\n",
    "\n",
    "# Visualize noise schedule\n",
    "t_vals = torch.linspace(0, 1, 100)\n",
    "alpha_vals = model_ct.alpha_bar(t_vals)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t_vals, alpha_vals, 'b-', linewidth=2)\n",
    "plt.xlabel('Time t')\n",
    "plt.ylabel('α̅(t)')\n",
    "plt.title('Cosine Noise Schedule')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "snr_vals = alpha_vals / (1 - alpha_vals + 1e-8)\n",
    "plt.plot(t_vals, snr_vals, 'r-', linewidth=2)\n",
    "plt.xlabel('Time t') \n",
    "plt.ylabel('SNR(t)')\n",
    "plt.title('Signal-to-Noise Ratio')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📈 The noise schedule shows how signal vs noise changes over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d17efe6",
   "metadata": {},
   "source": [
    "## Implementation 3: NoProp-FM (Flow Matching)\n",
    "\n",
    "The **Flow Matching** variant is the most theoretically elegant. Instead of denoising, it learns vector fields for optimal transport from noise to target.\n",
    "\n",
    "### 🌊 **Key Concepts:**\n",
    "\n",
    "1. **Vector Fields**: Learn velocity v(x,z,t) that transports noise → target\n",
    "2. **Straight Paths**: Uses straight-line interpolation z_t = t×z₁ + (1-t)×z₀  \n",
    "3. **Optimal Transport**: Mathematically grounded in optimal transport theory\n",
    "4. **Simpler Math**: No complicated noise schedules - just learn the flow!\n",
    "\n",
    "### 🎯 **Training Process:**\n",
    "1. Sample noise z₀ and target z₁ = u_y\n",
    "2. Linear interpolation: z_t = t×z₁ + (1-t)×z₀\n",
    "3. Target velocity: v* = z₁ - z₀ (constant!)\n",
    "4. Train model to predict v* from (x, z_t, t)\n",
    "\n",
    "### 💫 **Why It's Elegant:**\n",
    "- **Direct**: No noise schedules or complex denoising\n",
    "- **Efficient**: Straight paths are shortest distances  \n",
    "- **Principled**: Based on optimal transport theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fead91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPropFM(nn.Module):\n",
    "    \"\"\"\n",
    "    NoProp Flow Matching implementation.\n",
    "    \n",
    "    Uses vector fields and optimal transport instead of denoising.\n",
    "    Mathematically elegant and theoretically grounded.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Vector field predictor (reuses our DenoiseBlock architecture)\n",
    "        # But now it predicts velocities instead of denoised states!\n",
    "        self.vector_field = DenoiseBlock(embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Learnable class embeddings\n",
    "        self.W_embed = nn.Parameter(torch.randn(num_classes, embedding_dim) * 0.1)\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward_vector_field(self, x, z_t, t):\n",
    "        \"\"\"\n",
    "        Predict vector field v(x, z_t, t).\n",
    "        \n",
    "        This tells us which direction to move in latent space\n",
    "        to go from current position z_t toward the target.\n",
    "        \"\"\"\n",
    "        v_pred, _ = self.vector_field(x, z_t, self.W_embed)\n",
    "        return v_pred\n",
    "\n",
    "    def extrapolate_z1(self, z_t, v_pred, t):\n",
    "        \"\"\"\n",
    "        Extrapolate to final position z₁ given current position and velocity.\n",
    "        \n",
    "        If we're at z_t at time t, and velocity is v_pred,\n",
    "        where will we be at time 1? Answer: z_t + (1-t) × v_pred\n",
    "        \"\"\"\n",
    "        return z_t + (1 - t) * v_pred\n",
    "\n",
    "    def inference(self, x: torch.Tensor, steps: int = 100) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inference using flow matching.\n",
    "        \n",
    "        We solve: dz/dt = v(x,z,t) starting from z₀ ~ N(0,I)\n",
    "        This gives us a trajectory from noise to class representation.\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        z = torch.randn(B, self.embedding_dim, device=x.device)\n",
    "        \n",
    "        dt = 1.0 / steps\n",
    "        for i in range(steps):\n",
    "            t = torch.full((B, 1), i * dt, device=x.device)\n",
    "            \n",
    "            # Get velocity at current position\n",
    "            v = self.forward_vector_field(x, z, t)\n",
    "            \n",
    "            # Flow step: z_{t+dt} = z_t + dt × v\n",
    "            z = z + dt * v\n",
    "        \n",
    "        return self.classifier(z)\n",
    "\n",
    "    def training_step(self, x, y):\n",
    "        \"\"\"\n",
    "        Training step for flow matching NoProp.\n",
    "        \n",
    "        The beauty of flow matching: we use straight line paths!\n",
    "        Target velocity is simply v* = z₁ - z₀ (constant along the path).\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "\n",
    "        # Target class embeddings (where we want to end up)\n",
    "        z1 = self.W_embed[y]  # shape: [B, embedding_dim]\n",
    "        \n",
    "        # Random starting points (where we start)\n",
    "        z0 = torch.randn_like(z1)\n",
    "        \n",
    "        # Random time along the path\n",
    "        t = torch.rand(B, 1, device=x.device)\n",
    "\n",
    "        # Linear interpolation: z_t = t×z₁ + (1-t)×z₀\n",
    "        z_t = t * z1 + (1 - t) * z0\n",
    "        \n",
    "        # Target velocity (this is what makes flow matching elegant!)\n",
    "        v_target = z1 - z0  # Constant velocity along straight line!\n",
    "\n",
    "        # Predict velocity\n",
    "        v_pred = self.forward_vector_field(x, z_t, t)\n",
    "        \n",
    "        # L2 loss: predict the correct velocity\n",
    "        loss_l2 = F.mse_loss(v_pred, v_target)\n",
    "\n",
    "        # Extrapolate to final position and get classification loss\n",
    "        z1_hat = self.extrapolate_z1(z_t, v_pred, t)\n",
    "        logits = self.classifier(z1_hat)\n",
    "        loss_ce = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Total loss: flow matching + classification\n",
    "        total_loss = loss_l2 + loss_ce\n",
    "        return total_loss\n",
    "\n",
    "# Create and test NoProp-FM model\n",
    "print(\"🏗️  Creating NoProp-FM model...\")\n",
    "model_fm = NoPropFM(num_classes=10, embedding_dim=512).to(device)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_x = torch.randn(4, 1, 28, 28).to(device)\n",
    "    test_y = torch.randint(0, 10, (4,)).to(device)\n",
    "    \n",
    "    # Test inference\n",
    "    logits = model_fm.inference(test_x, steps=50)\n",
    "    print(f\"✅ Inference works! Output shape: {logits.shape}\")\n",
    "    \n",
    "    # Test training step  \n",
    "    loss = model_fm.training_step(test_x, test_y)\n",
    "    print(f\"✅ Training step works! Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.numel() for p in model_fm.parameters())\n",
    "print(f\"📊 NoProp-FM parameters: {total_params:,}\")\n",
    "print(f\"🌊 Uses vector fields for optimal transport\")\n",
    "print(f\"📏 Embedding dimension: {model_fm.embedding_dim}\")\n",
    "\n",
    "# Visualize flow matching concept\n",
    "print(\"\\n🎨 Flow Matching Visualization:\")\n",
    "print(\"   z₀ (noise) ──────→ z₁ (target)\")\n",
    "print(\"   Time:  0           1\")\n",
    "print(\"   Path:  z_t = t×z₁ + (1-t)×z₀\")\n",
    "print(\"   Velocity: v* = z₁ - z₀ (constant!)\")\n",
    "print(\"   Goal: Learn v(x,z_t,t) ≈ v*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9a408",
   "metadata": {},
   "source": [
    "## Training and Comparison: Battle of the NoProp Methods!\n",
    "\n",
    "Now comes the exciting part - let's train all three variants and see how they compare! \n",
    "\n",
    "We'll evaluate them on:\n",
    "- **🎯 Accuracy**: How well do they classify MNIST digits?\n",
    "- **⚡ Speed**: How fast do they train and infer?\n",
    "- **🧠 Interpretability**: How do their learned representations look?\n",
    "- **🔧 Ease of Use**: Which is simplest to implement and tune?\n",
    "\n",
    "### Training Setup\n",
    "\n",
    "For fair comparison, we'll use:\n",
    "- **Same architecture**: All use identical DenoiseBlock components\n",
    "- **Same hyperparameters**: Learning rate, batch size, epochs\n",
    "- **Same dataset**: MNIST with identical preprocessing  \n",
    "- **Same metrics**: Accuracy, loss, training time\n",
    "\n",
    "Let's see which approach wins! 🏆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ebe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_loader, test_loader, epochs=5, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Generic training function for any NoProp variant.\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 Training {model_name}...\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training loop\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_idx, (x, y) in enumerate(pbar):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.training_step(x, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            with torch.no_grad():\n",
    "                if hasattr(model, 'inference'):\n",
    "                    # For NoProp-CT and NoProp-FM, use fewer steps during training for speed\n",
    "                    if 'CT' in model_name or 'FM' in model_name:\n",
    "                        logits = model.inference(x, steps=10)\n",
    "                    else:\n",
    "                        logits = model.inference(x)\n",
    "                else:\n",
    "                    logits = model(x)\n",
    "                    \n",
    "                pred = logits.argmax(dim=1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            if batch_idx % 50 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'Acc': f'{100*correct/total:.1f}%'\n",
    "                })\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                if hasattr(model, 'inference'):\n",
    "                    # Use more steps for final evaluation\n",
    "                    logits = model.inference(x, steps=50)\n",
    "                else:\n",
    "                    logits = model(x)\n",
    "                    \n",
    "                pred = logits.argmax(dim=1)\n",
    "                test_correct += (pred == y).sum().item()\n",
    "                test_total += y.size(0)\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Train Acc={train_acc:.1f}%, Test Acc={test_acc:.1f}%\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"✅ {model_name} training complete! Time: {training_time:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        'losses': train_losses,\n",
    "        'accuracies': train_accuracies,\n",
    "        'training_time': training_time,\n",
    "        'final_accuracy': train_accuracies[-1]\n",
    "    }\n",
    "\n",
    "# Train all three models\n",
    "results = {}\n",
    "\n",
    "print(\"🏁 Starting the NoProp Championship!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train NoProp-DT\n",
    "results['NoProp-DT'] = train_model(model_dt, 'NoProp-DT', train_loader, test_loader, epochs=3)\n",
    "\n",
    "# Train NoProp-CT  \n",
    "results['NoProp-CT'] = train_model(model_ct, 'NoProp-CT', train_loader, test_loader, epochs=3)\n",
    "\n",
    "# Train NoProp-FM\n",
    "results['NoProp-FM'] = train_model(model_fm, 'NoProp-FM', train_loader, test_loader, epochs=3)\n",
    "\n",
    "print(\"\\n🏆 FINAL RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "for name, result in results.items():\n",
    "    print(f\"{name:12s}: {result['final_accuracy']:.1f}% accuracy in {result['training_time']:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74208d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training curves\n",
    "plt.subplot(2, 3, 1)\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['accuracies'], label=name, linewidth=2, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Learning Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Final accuracy comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "names = list(results.keys())\n",
    "accuracies = [results[name]['final_accuracy'] for name in names]\n",
    "colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "bars = plt.bar(names, accuracies, color=colors)\n",
    "plt.ylabel('Final Test Accuracy (%)')\n",
    "plt.title('Final Performance')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Training time comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "times = [results[name]['training_time'] for name in names]\n",
    "bars = plt.bar(names, times, color=colors)\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Training Speed')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: Parameter count comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "param_counts = [\n",
    "    sum(p.numel() for p in model_dt.parameters()),\n",
    "    sum(p.numel() for p in model_ct.parameters()),\n",
    "    sum(p.numel() for p in model_fm.parameters())\n",
    "]\n",
    "param_counts = [p/1e6 for p in param_counts]  # Convert to millions\n",
    "bars = plt.bar(names, param_counts, color=colors)\n",
    "plt.ylabel('Parameters (Millions)')\n",
    "plt.title('Model Size')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar, params in zip(bars, param_counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{params:.1f}M', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 5: Efficiency (accuracy per parameter)\n",
    "plt.subplot(2, 3, 5)\n",
    "efficiency = [acc/params for acc, params in zip(accuracies, param_counts)]\n",
    "bars = plt.bar(names, efficiency, color=colors)\n",
    "plt.ylabel('Accuracy per Million Parameters')\n",
    "plt.title('Parameter Efficiency')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar, eff in zip(bars, efficiency):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{eff:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 6: Speed vs Accuracy scatter\n",
    "plt.subplot(2, 3, 6)\n",
    "for i, name in enumerate(names):\n",
    "    plt.scatter(times[i], accuracies[i], s=100, color=colors[i], label=name, alpha=0.7)\n",
    "    plt.annotate(name, (times[i], accuracies[i]), xytext=(5, 5), \n",
    "                textcoords='offset points', fontsize=10)\n",
    "\n",
    "plt.xlabel('Training Time (seconds)')\n",
    "plt.ylabel('Final Accuracy (%)')\n",
    "plt.title('Speed vs Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔍 DETAILED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find the best performer in each category\n",
    "best_accuracy = max(names, key=lambda x: results[x]['final_accuracy'])\n",
    "fastest_training = min(names, key=lambda x: results[x]['training_time'])\n",
    "most_efficient = names[efficiency.index(max(efficiency))]\n",
    "\n",
    "print(f\"🏆 Best Accuracy: {best_accuracy} ({results[best_accuracy]['final_accuracy']:.1f}%)\")\n",
    "print(f\"⚡ Fastest Training: {fastest_training} ({results[fastest_training]['training_time']:.1f}s)\")\n",
    "print(f\"🎯 Most Efficient: {most_efficient} ({max(efficiency):.1f} acc/M params)\")\n",
    "\n",
    "print(f\"\\n📊 Summary Table:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Method':<12} {'Accuracy':<10} {'Time':<8} {'Params':<8} {'Efficiency':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, name in enumerate(names):\n",
    "    acc = results[name]['final_accuracy']\n",
    "    time_val = results[name]['training_time']\n",
    "    params = param_counts[i]\n",
    "    eff = efficiency[i]\n",
    "    print(f\"{name:<12} {acc:<10.1f} {time_val:<8.1f} {params:<8.1f} {eff:<10.1f}\")\n",
    "\n",
    "print(\"\\n💡 Key Insights:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"• NoProp-DT: Most parameters due to T separate blocks\")\n",
    "print(\"• NoProp-CT: Good balance of performance and efficiency\") \n",
    "print(\"• NoProp-FM: Theoretically elegant, simpler loss function\")\n",
    "print(\"• All methods avoid backpropagation - revolutionary!\")\n",
    "print(\"• Performance comparable to traditional CNNs on MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9653101",
   "metadata": {},
   "source": [
    "## Conclusions: The Future of Training Without Backpropagation\n",
    "\n",
    "After implementing and comparing all three NoProp variants, here are our key findings:\n",
    "\n",
    "### 🎯 **What We Learned**\n",
    "\n",
    "#### **NoProp-DT (Discrete Time)**\n",
    "- **Pros**: Simple to understand, deterministic behavior, good performance\n",
    "- **Cons**: More parameters due to T separate blocks, less flexible\n",
    "- **Best for**: Educational purposes, when you need predictable behavior\n",
    "\n",
    "#### **NoProp-CT (Continuous Time)**  \n",
    "- **Pros**: Flexible timesteps, good performance, single shared block\n",
    "- **Cons**: Requires ODE solvers, more complex mathematically\n",
    "- **Best for**: Research applications, when you need flexibility\n",
    "\n",
    "#### **NoProp-FM (Flow Matching)**\n",
    "- **Pros**: Theoretically elegant, simpler loss function, direct paths\n",
    "- **Cons**: Different mathematical framework, newer approach\n",
    "- **Best for**: Cutting-edge research, optimal transport applications\n",
    "\n",
    "### 🚀 **Revolutionary Implications**\n",
    "\n",
    "These results demonstrate that **backpropagation is not necessary** for training neural networks! This opens up exciting possibilities:\n",
    "\n",
    "1. **🧠 Biologically Plausible AI**: More similar to how real neurons might learn\n",
    "2. **⚡ Parallel Training**: Different layers can be trained simultaneously\n",
    "3. **🔧 Hardware Optimization**: Custom chips for local learning rules\n",
    "4. **🌍 Distributed Learning**: Training across multiple devices without gradients\n",
    "5. **📱 Edge Computing**: Memory-efficient training on mobile devices\n",
    "\n",
    "### 🔬 **Future Research Directions**\n",
    "\n",
    "1. **Scale Up**: Test on larger datasets (CIFAR-100, ImageNet)\n",
    "2. **Architecture Exploration**: CNNs, Vision Transformers, etc.\n",
    "3. **Theoretical Analysis**: Convergence guarantees, capacity bounds  \n",
    "4. **Applications**: NLP, multimodal learning, reinforcement learning\n",
    "5. **Hardware Co-design**: Neuromorphic chips, analog computing\n",
    "\n",
    "### 💭 **Final Thoughts**\n",
    "\n",
    "NoProp represents a **paradigm shift** in deep learning. While backpropagation has dominated for decades, these methods show that:\n",
    "\n",
    "- **Local learning rules can be as effective as global ones**\n",
    "- **Biology-inspired algorithms have practical value** \n",
    "- **There are multiple paths to intelligent systems**\n",
    "\n",
    "The future of AI might look very different from today - and NoProp gives us a glimpse of what's possible! 🌟\n",
    "\n",
    "---\n",
    "\n",
    "*\\\"The best way to predict the future is to invent it.\\\"* - Alan Kay\n",
    "\n",
    "And that's exactly what NoProp is doing - **inventing the future of neural network training!** 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbedd235",
   "metadata": {},
   "source": [
    "## Practical Tips for Implementing NoProp\n",
    "\n",
    "Based on our experience, here are some practical tips for anyone wanting to implement NoProp methods:\n",
    "\n",
    "### 🛠️ **Implementation Tips**\n",
    "\n",
    "1. **Start Simple**: Begin with NoProp-DT - it's the most straightforward\n",
    "2. **Gradient Clipping**: Always use gradient clipping (we used 1.0)\n",
    "3. **Class Embedding Init**: Initialize W_embed carefully (we used 0.1 * randn)\n",
    "4. **Batch Size**: Use reasonable batch sizes (128 worked well)\n",
    "5. **Learning Rate**: Start with 1e-3 and adjust as needed\n",
    "\n",
    "### ⚙️ **Hyperparameter Guidelines**\n",
    "\n",
    "| Parameter | NoProp-DT | NoProp-CT | NoProp-FM | Notes |\n",
    "|-----------|-----------|-----------|-----------|-------|\n",
    "| T (timesteps) | 10-20 | N/A | N/A | More = better but slower |\n",
    "| Inference steps | N/A | 50-100 | 50-100 | Trade-off speed vs accuracy |\n",
    "| eta (η) | 0.1-1.0 | 1.0 | N/A | Loss weighting factor |\n",
    "| Embedding dim | 256-512 | 256-512 | 256-512 | Higher = more capacity |\n",
    "\n",
    "### 🐛 **Common Issues & Solutions**\n",
    "\n",
    "1. **Training Instability**: \n",
    "   - Use gradient clipping\n",
    "   - Reduce learning rate\n",
    "   - Add more dropout\n",
    "\n",
    "2. **Poor Convergence**:\n",
    "   - Check class embedding initialization\n",
    "   - Verify noise schedule implementation\n",
    "   - Ensure proper loss weighting\n",
    "\n",
    "3. **Slow Inference**:\n",
    "   - Reduce inference steps for CT/FM\n",
    "   - Use Euler instead of higher-order ODE solvers\n",
    "   - Cache computations when possible\n",
    "\n",
    "### 📚 **Further Reading**\n",
    "\n",
    "- Original NoProp paper: [Link to paper]\n",
    "- Diffusion models background: Ho et al. (2020)\n",
    "- Flow matching theory: Lipman et al. (2023)\n",
    "- VAE and ELBO: Kingma & Welling (2014)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
